{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import pywt\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "data_path=\"D:\\\\Project ICOD\\\\256_ObjectCategories\"\n",
    "files = os.listdir(data_path)\n",
    "cat=[]\n",
    "for name in files:\n",
    "    cat+=[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.ak47\n",
      "002.american-flag\n",
      "003.backpack\n",
      "004.baseball-bat\n",
      "005.baseball-glove\n",
      "006.basketball-hoop\n",
      "007.bat\n",
      "008.bathtub\n",
      "009.bear\n",
      "010.beer-mug\n",
      "011.billiards\n",
      "012.binoculars\n",
      "013.birdbath\n",
      "014.blimp\n",
      "015.bonsai-101\n",
      "016.boom-box\n",
      "017.bowling-ball\n",
      "018.bowling-pin\n",
      "019.boxing-glove\n",
      "020.brain-101\n",
      "021.breadmaker\n",
      "022.buddha-101\n",
      "023.bulldozer\n",
      "024.butterfly\n",
      "025.cactus\n",
      "026.cake\n",
      "027.calculator\n",
      "028.camel\n",
      "029.cannon\n",
      "030.canoe\n",
      "031.car-tire\n",
      "032.cartman\n",
      "033.cd\n",
      "034.centipede\n",
      "035.cereal-box\n",
      "036.chandelier-101\n",
      "037.chess-board\n",
      "038.chimp\n",
      "039.chopsticks\n",
      "040.cockroach\n",
      "041.coffee-mug\n",
      "042.coffin\n",
      "043.coin\n",
      "044.comet\n",
      "045.computer-keyboard\n",
      "046.computer-monitor\n",
      "047.computer-mouse\n",
      "048.conch\n",
      "049.cormorant\n",
      "050.covered-wagon\n",
      "051.cowboy-hat\n",
      "052.crab-101\n",
      "053.desk-globe\n",
      "054.diamond-ring\n",
      "055.dice\n",
      "056.dog\n",
      "057.dolphin-101\n",
      "058.doorknob\n",
      "059.drinking-straw\n",
      "060.duck\n",
      "061.dumb-bell\n",
      "062.eiffel-tower\n",
      "063.electric-guitar-101\n",
      "064.elephant-101\n",
      "065.elk\n",
      "066.ewer-101\n",
      "067.eyeglasses\n",
      "068.fern\n",
      "069.fighter-jet\n",
      "070.fire-extinguisher\n",
      "071.fire-hydrant\n",
      "072.fire-truck\n",
      "073.fireworks\n",
      "074.flashlight\n",
      "075.floppy-disk\n",
      "076.football-helmet\n",
      "077.french-horn\n",
      "078.fried-egg\n",
      "079.frisbee\n",
      "080.frog\n",
      "081.frying-pan\n",
      "082.galaxy\n",
      "083.gas-pump\n",
      "084.giraffe\n",
      "085.goat\n",
      "086.golden-gate-bridge\n",
      "087.goldfish\n",
      "088.golf-ball\n",
      "089.goose\n",
      "090.gorilla\n",
      "091.grand-piano-101\n",
      "092.grapes\n",
      "093.grasshopper\n",
      "094.guitar-pick\n",
      "095.hamburger\n",
      "096.hammock\n",
      "097.harmonica\n",
      "098.harp\n",
      "099.harpsichord\n",
      "100.hawksbill-101\n",
      "101.head-phones\n",
      "102.helicopter-101\n",
      "103.hibiscus\n",
      "104.homer-simpson\n",
      "105.horse\n",
      "106.horseshoe-crab\n",
      "107.hot-air-balloon\n",
      "108.hot-dog\n",
      "109.hot-tub\n",
      "110.hourglass\n",
      "111.house-fly\n",
      "112.human-skeleton\n",
      "113.hummingbird\n",
      "114.ibis-101\n",
      "115.ice-cream-cone\n",
      "116.iguana\n",
      "117.ipod\n",
      "118.iris\n",
      "119.jesus-christ\n",
      "120.joy-stick\n",
      "121.kangaroo-101\n",
      "122.kayak\n",
      "123.ketch-101\n",
      "124.killer-whale\n",
      "125.knife\n",
      "126.ladder\n",
      "127.laptop-101\n",
      "128.lathe\n",
      "129.leopards-101\n",
      "130.license-plate\n",
      "131.lightbulb\n",
      "132.light-house\n",
      "133.lightning\n",
      "134.llama-101\n",
      "135.mailbox\n",
      "136.mandolin\n",
      "137.mars\n",
      "138.mattress\n",
      "139.megaphone\n",
      "140.menorah-101\n",
      "141.microscope\n",
      "142.microwave\n",
      "143.minaret\n",
      "144.minotaur\n",
      "145.motorbikes-101\n",
      "146.mountain-bike\n",
      "147.mushroom\n",
      "148.mussels\n",
      "149.necktie\n",
      "150.octopus\n",
      "151.ostrich\n",
      "152.owl\n",
      "153.palm-pilot\n",
      "154.palm-tree\n",
      "155.paperclip\n",
      "156.paper-shredder\n",
      "157.pci-card\n",
      "158.penguin\n",
      "159.people\n",
      "160.pez-dispenser\n",
      "161.photocopier\n",
      "162.picnic-table\n",
      "163.playing-card\n",
      "164.porcupine\n",
      "165.pram\n",
      "166.praying-mantis\n",
      "167.pyramid\n",
      "168.raccoon\n",
      "169.radio-telescope\n",
      "170.rainbow\n",
      "171.refrigerator\n",
      "172.revolver-101\n",
      "173.rifle\n",
      "174.rotary-phone\n",
      "175.roulette-wheel\n",
      "176.saddle\n",
      "177.saturn\n",
      "178.school-bus\n",
      "179.scorpion-101\n",
      "180.screwdriver\n",
      "181.segway\n",
      "182.self-propelled-lawn-mower\n",
      "183.sextant\n",
      "184.sheet-music\n",
      "185.skateboard\n",
      "186.skunk\n",
      "187.skyscraper\n",
      "188.smokestack\n",
      "189.snail\n",
      "190.snake\n",
      "191.sneaker\n",
      "192.snowmobile\n",
      "193.soccer-ball\n",
      "194.socks\n",
      "195.soda-can\n",
      "196.spaghetti\n",
      "197.speed-boat\n",
      "198.spider\n",
      "199.spoon\n",
      "200.stained-glass\n",
      "201.starfish-101\n",
      "202.steering-wheel\n",
      "203.stirrups\n",
      "204.sunflower-101\n",
      "205.superman\n",
      "206.sushi\n",
      "207.swan\n",
      "208.swiss-army-knife\n",
      "209.sword\n",
      "210.syringe\n",
      "211.tambourine\n",
      "212.teapot\n",
      "213.teddy-bear\n",
      "214.teepee\n",
      "215.telephone-box\n",
      "216.tennis-ball\n",
      "217.tennis-court\n",
      "218.tennis-racket\n",
      "219.theodolite\n",
      "220.toaster\n",
      "221.tomato\n",
      "222.tombstone\n",
      "223.top-hat\n",
      "224.touring-bike\n",
      "225.tower-pisa\n",
      "226.traffic-light\n",
      "227.treadmill\n",
      "228.triceratops\n",
      "229.tricycle\n",
      "230.trilobite-101\n",
      "231.tripod\n",
      "232.t-shirt\n",
      "233.tuning-fork\n",
      "234.tweezer\n",
      "235.umbrella-101\n",
      "236.unicorn\n",
      "237.vcr\n",
      "238.video-projector\n",
      "239.washing-machine\n",
      "240.watch-101\n",
      "241.waterfall\n",
      "242.watermelon\n",
      "243.welding-mask\n",
      "244.wheelbarrow\n",
      "245.windmill\n",
      "246.wine-bottle\n",
      "247.xylophone\n",
      "248.yarmulke\n",
      "249.yo-yo\n",
      "250.zebra\n",
      "251.airplanes-101\n",
      "252.car-side-101\n",
      "253.faces-easy-101\n",
      "254.greyhound\n",
      "255.tennis-shoes\n",
      "256.toad\n",
      "257.clutter\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.feature import hog\n",
    "fd_trainH=[]\n",
    "fd_trainV=[]\n",
    "fd_trainD=[]\n",
    "fd_trainA=[]\n",
    "\n",
    "for i in cat:\n",
    "    path=os.path.join(data_path,i)\n",
    "    print(i)\n",
    "    for img in os.listdir(path):\n",
    "        image=os.path.join(path,img)\n",
    "         \n",
    "        train_data=cv2.imread(image)\n",
    "        \n",
    "        #Principal component analysis\n",
    "        blue,green,red = cv2.split(train_data)\n",
    "        #it will split the original image into Blue, Green and Red arrays.\n",
    "        #initialize PCA with first 50 principal components\n",
    "        pca = PCA(50)\n",
    "\n",
    "        #Applying to red channel and then applying inverse transform to transformed array.\n",
    "        red_transformed = pca.fit_transform(red)\n",
    "        red_inverted = pca.inverse_transform(red_transformed)\n",
    "\n",
    "        #Applying to Green channel and then applying inverse transform to transformed array.\n",
    "        green_transformed = pca.fit_transform(green)\n",
    "        green_inverted = pca.inverse_transform(green_transformed)\n",
    "\n",
    "        #Applying to Blue channel and then applying inverse transform to transformed array.\n",
    "        blue_transformed = pca.fit_transform(blue)\n",
    "        blue_inverted = pca.inverse_transform(blue_transformed)\n",
    "\n",
    "        img_compressed = (np.dstack((red_inverted, green_inverted, blue_inverted))).astype(np.uint8)\n",
    "        img=cv2.cvtColor(img_compressed,cv2.COLOR_BGR2GRAY)\n",
    "        img = resize(img, (128, 64))\n",
    "        \n",
    "        #Daubechies Wavelet\n",
    "        coeffs2 = pywt.dwt2(img, 'db2')\n",
    "        LL, (LH, HL, HH) = coeffs2\n",
    "        \n",
    "        # Approximation\n",
    "        fdA, hog_image = hog(LL, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "        fd_trainA +=[ np.append(fdA,cat.index(i))]\n",
    "        \n",
    "        # Horizontal\n",
    "        fdH, hog_image = hog(LH, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "        fd_trainH +=[ np.append(fdH,cat.index(i))]\n",
    "        \n",
    "        #Vertical\n",
    "        fdV, hog_image = hog(HL, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "        fd_trainV +=[ np.append(fdV,cat.index(i))]\n",
    "        \n",
    "        #diagonal\n",
    "        fdD, hog_image = hog(HH, orientations=9, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)\n",
    "        fd_trainD +=[ np.append(fdD,cat.index(i))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('HOG_PCS50_dbWaveletA_3070_256.csv',fd_trainA, delimiter=',',fmt='%f')\n",
    "#np.savetxt('HOG_PCS50_dbWaveletH_3070_101.csv',fd_trainH, delimiter=',',fmt='%f')\n",
    "#np.savetxt('HOG_PCS50_dbWaveletV_3070_101.csv',fd_trainV, delimiter=',',fmt='%f')\n",
    "#np.savetxt('HOG_PCS50_dbWaveletD_3070_101.csv',fd_trainD, delimiter=',',fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Leearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.326475</td>\n",
       "      <td>0.125023</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.031084</td>\n",
       "      <td>0.071291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018284</td>\n",
       "      <td>0.109779</td>\n",
       "      <td>0.326475</td>\n",
       "      <td>0.326475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284152</td>\n",
       "      <td>0.265272</td>\n",
       "      <td>0.158737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078903</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.023648</td>\n",
       "      <td>0.080222</td>\n",
       "      <td>0.284152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237405</td>\n",
       "      <td>0.237405</td>\n",
       "      <td>0.064029</td>\n",
       "      <td>0.067817</td>\n",
       "      <td>0.108925</td>\n",
       "      <td>0.015440</td>\n",
       "      <td>0.088530</td>\n",
       "      <td>0.141404</td>\n",
       "      <td>0.237405</td>\n",
       "      <td>0.237405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264644</td>\n",
       "      <td>0.071581</td>\n",
       "      <td>0.140366</td>\n",
       "      <td>0.092822</td>\n",
       "      <td>0.136848</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.090780</td>\n",
       "      <td>0.264644</td>\n",
       "      <td>0.264644</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.249994</td>\n",
       "      <td>0.111255</td>\n",
       "      <td>0.168295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089669</td>\n",
       "      <td>0.122059</td>\n",
       "      <td>0.139760</td>\n",
       "      <td>0.249994</td>\n",
       "      <td>0.233415</td>\n",
       "      <td>0.215573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255045</td>\n",
       "      <td>0.178091</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>0.064833</td>\n",
       "      <td>0.033389</td>\n",
       "      <td>0.091368</td>\n",
       "      <td>0.163230</td>\n",
       "      <td>0.255045</td>\n",
       "      <td>0.255045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.254359</td>\n",
       "      <td>0.137140</td>\n",
       "      <td>0.074012</td>\n",
       "      <td>0.026266</td>\n",
       "      <td>0.156097</td>\n",
       "      <td>0.046091</td>\n",
       "      <td>0.086943</td>\n",
       "      <td>0.069838</td>\n",
       "      <td>0.070551</td>\n",
       "      <td>0.124353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188440</td>\n",
       "      <td>0.238937</td>\n",
       "      <td>0.292161</td>\n",
       "      <td>0.260253</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.061629</td>\n",
       "      <td>0.088481</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.060679</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231622</td>\n",
       "      <td>0.214162</td>\n",
       "      <td>0.245719</td>\n",
       "      <td>0.255229</td>\n",
       "      <td>0.170384</td>\n",
       "      <td>0.149270</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>0.178430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078859</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>0.025235</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>0.060329</td>\n",
       "      <td>0.010744</td>\n",
       "      <td>0.040548</td>\n",
       "      <td>0.063549</td>\n",
       "      <td>0.147744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30602</th>\n",
       "      <td>0.178567</td>\n",
       "      <td>0.236483</td>\n",
       "      <td>0.194328</td>\n",
       "      <td>0.176509</td>\n",
       "      <td>0.226954</td>\n",
       "      <td>0.136709</td>\n",
       "      <td>0.048391</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>0.040355</td>\n",
       "      <td>0.236483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284851</td>\n",
       "      <td>0.284851</td>\n",
       "      <td>0.089878</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>0.032474</td>\n",
       "      <td>0.054724</td>\n",
       "      <td>0.027559</td>\n",
       "      <td>0.108581</td>\n",
       "      <td>0.284851</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30603</th>\n",
       "      <td>0.196988</td>\n",
       "      <td>0.301829</td>\n",
       "      <td>0.301829</td>\n",
       "      <td>0.280068</td>\n",
       "      <td>0.268266</td>\n",
       "      <td>0.147050</td>\n",
       "      <td>0.094916</td>\n",
       "      <td>0.120153</td>\n",
       "      <td>0.125121</td>\n",
       "      <td>0.301829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063698</td>\n",
       "      <td>0.058402</td>\n",
       "      <td>0.019607</td>\n",
       "      <td>0.085398</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.219262</td>\n",
       "      <td>0.056280</td>\n",
       "      <td>0.051949</td>\n",
       "      <td>0.088712</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30604</th>\n",
       "      <td>0.042770</td>\n",
       "      <td>0.029441</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.023573</td>\n",
       "      <td>0.035125</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.039170</td>\n",
       "      <td>0.042064</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216022</td>\n",
       "      <td>0.137231</td>\n",
       "      <td>0.101798</td>\n",
       "      <td>0.140992</td>\n",
       "      <td>0.197291</td>\n",
       "      <td>0.260642</td>\n",
       "      <td>0.163201</td>\n",
       "      <td>0.212968</td>\n",
       "      <td>0.260642</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30605</th>\n",
       "      <td>0.236787</td>\n",
       "      <td>0.210014</td>\n",
       "      <td>0.104977</td>\n",
       "      <td>0.147061</td>\n",
       "      <td>0.236787</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>0.053867</td>\n",
       "      <td>0.118430</td>\n",
       "      <td>0.131130</td>\n",
       "      <td>0.236787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205036</td>\n",
       "      <td>0.088562</td>\n",
       "      <td>0.068206</td>\n",
       "      <td>0.049354</td>\n",
       "      <td>0.009044</td>\n",
       "      <td>0.081994</td>\n",
       "      <td>0.090595</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.164141</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30606</th>\n",
       "      <td>0.239435</td>\n",
       "      <td>0.077201</td>\n",
       "      <td>0.076604</td>\n",
       "      <td>0.119547</td>\n",
       "      <td>0.117582</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.171261</td>\n",
       "      <td>0.239435</td>\n",
       "      <td>0.147002</td>\n",
       "      <td>0.239435</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231506</td>\n",
       "      <td>0.204376</td>\n",
       "      <td>0.113707</td>\n",
       "      <td>0.208868</td>\n",
       "      <td>0.151251</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.078806</td>\n",
       "      <td>0.137188</td>\n",
       "      <td>0.176805</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30607 rows × 757 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.326475  0.125023  0.025134  0.031084  0.071291  0.000000  0.018284   \n",
       "1      0.237405  0.237405  0.064029  0.067817  0.108925  0.015440  0.088530   \n",
       "2      0.249994  0.111255  0.168295  0.000000  0.089669  0.122059  0.139760   \n",
       "3      0.254359  0.137140  0.074012  0.026266  0.156097  0.046091  0.086943   \n",
       "4      0.231622  0.214162  0.245719  0.255229  0.170384  0.149270  0.016762   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30602  0.178567  0.236483  0.194328  0.176509  0.226954  0.136709  0.048391   \n",
       "30603  0.196988  0.301829  0.301829  0.280068  0.268266  0.147050  0.094916   \n",
       "30604  0.042770  0.029441  0.002849  0.023573  0.035125  0.030902  0.006028   \n",
       "30605  0.236787  0.210014  0.104977  0.147061  0.236787  0.045746  0.053867   \n",
       "30606  0.239435  0.077201  0.076604  0.119547  0.117582  0.126100  0.171261   \n",
       "\n",
       "            7         8         9    ...       747       748       749  \\\n",
       "0      0.109779  0.326475  0.326475  ...  0.284152  0.265272  0.158737   \n",
       "1      0.141404  0.237405  0.237405  ...  0.264644  0.071581  0.140366   \n",
       "2      0.249994  0.233415  0.215573  ...  0.255045  0.178091  0.106855   \n",
       "3      0.069838  0.070551  0.124353  ...  0.188440  0.238937  0.292161   \n",
       "4      0.000000  0.032246  0.178430  ...  0.078859  0.049399  0.025235   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "30602  0.097439  0.040355  0.236483  ...  0.284851  0.284851  0.089878   \n",
       "30603  0.120153  0.125121  0.301829  ...  0.063698  0.058402  0.019607   \n",
       "30604  0.039170  0.042064  0.029633  ...  0.216022  0.137231  0.101798   \n",
       "30605  0.118430  0.131130  0.236787  ...  0.205036  0.088562  0.068206   \n",
       "30606  0.239435  0.147002  0.239435  ...  0.231506  0.204376  0.113707   \n",
       "\n",
       "            750       751       752       753       754       755    756  \n",
       "0      0.000000  0.078903  0.009825  0.023648  0.080222  0.284152    0.0  \n",
       "1      0.092822  0.136848  0.028615  0.090780  0.264644  0.264644    0.0  \n",
       "2      0.064833  0.033389  0.091368  0.163230  0.255045  0.255045    0.0  \n",
       "3      0.260253  0.027538  0.061629  0.088481  0.052727  0.060679    0.0  \n",
       "4      0.069025  0.060329  0.010744  0.040548  0.063549  0.147744    0.0  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "30602  0.023398  0.032474  0.054724  0.027559  0.108581  0.284851  256.0  \n",
       "30603  0.085398  0.265432  0.219262  0.056280  0.051949  0.088712  256.0  \n",
       "30604  0.140992  0.197291  0.260642  0.163201  0.212968  0.260642  256.0  \n",
       "30605  0.049354  0.009044  0.081994  0.090595  0.032800  0.164141  256.0  \n",
       "30606  0.208868  0.151251  0.046909  0.078806  0.137188  0.176805  256.0  \n",
       "\n",
       "[30607 rows x 757 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load the training dataset\n",
    "#images = pd.read_csv('array1.csv',header=None)\n",
    "images = pd.read_csv('HOG_PCS50_dbWaveletA_3070_256.csv',header=None)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.326475, 0.125023, 0.025134, ..., 0.023648, 0.080222, 0.284152],\n",
       "       [0.237405, 0.237405, 0.064029, ..., 0.09078 , 0.264644, 0.264644],\n",
       "       [0.249994, 0.111255, 0.168295, ..., 0.16323 , 0.255045, 0.255045],\n",
       "       ...,\n",
       "       [0.04277 , 0.029441, 0.002849, ..., 0.163201, 0.212968, 0.260642],\n",
       "       [0.236787, 0.210014, 0.104977, ..., 0.090595, 0.0328  , 0.164141],\n",
       "       [0.239435, 0.077201, 0.076604, ..., 0.078806, 0.137188, 0.176805]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,y=images[range(756)].values,images[756].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split data 15%-85% into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30607, 756)\n",
      "(9182, 756)\n",
      "(21425, 756)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 1\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:  [ 91.  95. 256. ...   0. 192. 231.]\n",
      "Actual labels:     [110.  16.  95. ...  58. 125. 231.]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Actual labels:    ' ,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.16261376896149357\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.06      0.03      0.04        65\n",
      "         1.0       0.10      0.03      0.05        63\n",
      "         2.0       0.25      0.21      0.23       113\n",
      "         3.0       0.04      0.01      0.02        88\n",
      "         4.0       0.11      0.15      0.13       103\n",
      "         5.0       0.00      0.00      0.00        60\n",
      "         6.0       0.00      0.00      0.00        71\n",
      "         7.0       0.05      0.08      0.06       161\n",
      "         8.0       0.10      0.01      0.02        73\n",
      "         9.0       0.09      0.03      0.04        69\n",
      "        10.0       0.07      0.18      0.10       193\n",
      "        11.0       0.17      0.33      0.22       156\n",
      "        12.0       0.25      0.01      0.03        74\n",
      "        13.0       0.07      0.03      0.04        63\n",
      "        14.0       0.20      0.20      0.20        90\n",
      "        15.0       0.23      0.05      0.08        65\n",
      "        16.0       0.26      0.14      0.19        76\n",
      "        17.0       0.11      0.08      0.09        65\n",
      "        18.0       0.03      0.01      0.02        90\n",
      "        19.0       0.60      0.05      0.09        64\n",
      "        20.0       0.26      0.38      0.31        96\n",
      "        21.0       0.21      0.42      0.28        65\n",
      "        22.0       0.00      0.00      0.00        83\n",
      "        23.0       0.04      0.01      0.02        80\n",
      "        24.0       0.00      0.00      0.00        86\n",
      "        25.0       0.09      0.01      0.02        79\n",
      "        26.0       0.07      0.03      0.04        69\n",
      "        27.0       0.02      0.01      0.01        71\n",
      "        28.0       0.04      0.01      0.02        72\n",
      "        29.0       0.00      0.00      0.00        69\n",
      "        30.0       0.12      0.10      0.11        63\n",
      "        31.0       0.08      0.14      0.10        59\n",
      "        32.0       0.22      0.29      0.25        70\n",
      "        33.0       0.00      0.00      0.00        73\n",
      "        34.0       0.04      0.02      0.02        61\n",
      "        35.0       0.17      0.13      0.15        78\n",
      "        36.0       0.11      0.12      0.12        82\n",
      "        37.0       0.00      0.00      0.00        77\n",
      "        38.0       0.00      0.00      0.00        57\n",
      "        39.0       0.02      0.01      0.01        89\n",
      "        40.0       0.20      0.12      0.15        60\n",
      "        41.0       0.05      0.02      0.03        59\n",
      "        42.0       0.07      0.14      0.10        76\n",
      "        43.0       0.08      0.12      0.10        73\n",
      "        44.0       0.00      0.00      0.00        65\n",
      "        45.0       0.27      0.32      0.29        97\n",
      "        46.0       0.08      0.06      0.07        63\n",
      "        47.0       0.00      0.00      0.00        79\n",
      "        48.0       0.15      0.02      0.04        82\n",
      "        49.0       0.07      0.06      0.07        62\n",
      "        50.0       0.22      0.28      0.25        76\n",
      "        51.0       0.00      0.00      0.00        65\n",
      "        52.0       0.35      0.12      0.18        65\n",
      "        53.0       0.10      0.24      0.14        75\n",
      "        54.0       0.00      0.00      0.00        69\n",
      "        55.0       0.00      0.00      0.00        72\n",
      "        56.0       0.04      0.03      0.03        66\n",
      "        57.0       0.02      0.02      0.02        60\n",
      "        58.0       0.05      0.02      0.03        55\n",
      "        59.0       0.00      0.00      0.00        67\n",
      "        60.0       0.00      0.00      0.00        75\n",
      "        61.0       0.41      0.15      0.22        59\n",
      "        62.0       0.25      0.01      0.02        99\n",
      "        63.0       0.09      0.13      0.11        90\n",
      "        64.0       0.03      0.01      0.02        72\n",
      "        65.0       0.31      0.13      0.18        62\n",
      "        66.0       0.31      0.15      0.20        62\n",
      "        67.0       0.00      0.00      0.00        82\n",
      "        68.0       0.05      0.01      0.02        69\n",
      "        69.0       0.04      0.04      0.04        53\n",
      "        70.0       0.05      0.01      0.02        72\n",
      "        71.0       0.13      0.17      0.15        82\n",
      "        72.0       0.00      0.00      0.00        67\n",
      "        73.0       0.05      0.03      0.03        80\n",
      "        74.0       0.38      0.09      0.15        65\n",
      "        75.0       0.17      0.02      0.03        62\n",
      "        76.0       0.14      0.07      0.10        67\n",
      "        77.0       0.08      0.01      0.03        67\n",
      "        78.0       0.20      0.12      0.15        72\n",
      "        79.0       0.00      0.00      0.00        82\n",
      "        80.0       0.17      0.06      0.09        67\n",
      "        81.0       0.16      0.09      0.12        54\n",
      "        82.0       0.00      0.00      0.00        71\n",
      "        83.0       0.00      0.00      0.00        52\n",
      "        84.0       0.00      0.00      0.00        84\n",
      "        85.0       0.05      0.02      0.03        55\n",
      "        86.0       0.06      0.02      0.02        65\n",
      "        87.0       0.03      0.02      0.02        65\n",
      "        88.0       0.05      0.04      0.04        73\n",
      "        89.0       0.02      0.05      0.03       145\n",
      "        90.0       0.24      0.21      0.22        63\n",
      "        91.0       0.01      0.03      0.02       140\n",
      "        92.0       0.04      0.04      0.04        73\n",
      "        93.0       0.42      0.48      0.45        69\n",
      "        94.0       0.09      0.05      0.06        63\n",
      "        95.0       0.05      0.19      0.08       189\n",
      "        96.0       0.22      0.03      0.05        69\n",
      "        97.0       0.22      0.11      0.14        75\n",
      "        98.0       0.23      0.05      0.08        61\n",
      "        99.0       0.12      0.04      0.06        67\n",
      "       100.0       0.09      0.07      0.08        97\n",
      "       101.0       0.11      0.11      0.11        63\n",
      "       102.0       0.10      0.04      0.05        81\n",
      "       103.0       0.02      0.02      0.02        62\n",
      "       104.0       0.04      0.12      0.06       190\n",
      "       105.0       0.00      0.00      0.00        58\n",
      "       106.0       0.19      0.09      0.12        64\n",
      "       107.0       0.00      0.00      0.00        64\n",
      "       108.0       0.09      0.14      0.11       103\n",
      "       109.0       0.22      0.19      0.20        59\n",
      "       110.0       0.11      0.03      0.05        59\n",
      "       111.0       0.20      0.06      0.09        66\n",
      "       112.0       0.05      0.04      0.04        83\n",
      "       113.0       0.05      0.09      0.06        78\n",
      "       114.0       0.00      0.00      0.00        69\n",
      "       115.0       0.00      0.00      0.00        74\n",
      "       116.0       0.13      0.14      0.13        80\n",
      "       117.0       0.00      0.00      0.00        81\n",
      "       118.0       0.03      0.02      0.02        62\n",
      "       119.0       0.04      0.06      0.05        89\n",
      "       120.0       0.07      0.03      0.05        58\n",
      "       121.0       0.03      0.01      0.02        69\n",
      "       122.0       0.36      0.44      0.40        81\n",
      "       123.0       0.00      0.00      0.00        70\n",
      "       124.0       0.02      0.02      0.02        61\n",
      "       125.0       0.04      0.04      0.04       179\n",
      "       126.0       0.30      0.19      0.24        93\n",
      "       127.0       0.03      0.01      0.02        69\n",
      "       128.0       0.19      0.74      0.30       129\n",
      "       129.0       0.30      0.26      0.28        65\n",
      "       130.0       0.22      0.12      0.16        65\n",
      "       131.0       0.09      0.22      0.13       121\n",
      "       132.0       0.13      0.08      0.10        98\n",
      "       133.0       0.06      0.04      0.05        82\n",
      "       134.0       0.00      0.00      0.00        71\n",
      "       135.0       0.03      0.02      0.02        63\n",
      "       136.0       0.24      0.30      0.27       107\n",
      "       137.0       0.18      0.14      0.16       141\n",
      "       138.0       0.20      0.01      0.03        67\n",
      "       139.0       0.40      0.22      0.28        65\n",
      "       140.0       0.05      0.01      0.02        90\n",
      "       141.0       0.26      0.31      0.28        70\n",
      "       142.0       0.11      0.06      0.07        89\n",
      "       143.0       0.00      0.00      0.00        56\n",
      "       144.0       0.51      0.95      0.67       558\n",
      "       145.0       0.33      0.03      0.06        63\n",
      "       146.0       0.02      0.10      0.04       126\n",
      "       147.0       0.02      0.03      0.03       121\n",
      "       148.0       0.25      0.06      0.10        80\n",
      "       149.0       0.02      0.01      0.02        74\n",
      "       150.0       0.09      0.05      0.06        80\n",
      "       151.0       0.03      0.02      0.03        83\n",
      "       152.0       0.11      0.01      0.03        67\n",
      "       153.0       0.05      0.03      0.04        73\n",
      "       154.0       0.00      0.00      0.00        62\n",
      "       155.0       0.36      0.22      0.27        69\n",
      "       156.0       0.00      0.00      0.00        74\n",
      "       157.0       0.08      0.08      0.08       104\n",
      "       158.0       0.03      0.05      0.04       147\n",
      "       159.0       0.16      0.14      0.15        56\n",
      "       160.0       0.11      0.13      0.12        68\n",
      "       161.0       0.00      0.00      0.00        64\n",
      "       162.0       0.00      0.00      0.00        61\n",
      "       163.0       0.14      0.04      0.06        75\n",
      "       164.0       0.00      0.00      0.00        62\n",
      "       165.0       0.08      0.07      0.08        54\n",
      "       166.0       0.14      0.03      0.05        61\n",
      "       167.0       0.00      0.00      0.00        98\n",
      "       168.0       0.05      0.03      0.04        64\n",
      "       169.0       0.12      0.03      0.04        75\n",
      "       170.0       0.06      0.08      0.07        51\n",
      "       171.0       0.13      0.19      0.16        64\n",
      "       172.0       0.00      0.00      0.00        78\n",
      "       173.0       0.16      0.09      0.11        56\n",
      "       174.0       0.15      0.03      0.06        59\n",
      "       175.0       0.20      0.18      0.19        78\n",
      "       176.0       0.61      0.18      0.28        78\n",
      "       177.0       0.00      0.00      0.00        73\n",
      "       178.0       0.00      0.00      0.00        49\n",
      "       179.0       0.00      0.00      0.00        75\n",
      "       180.0       0.05      0.04      0.05        67\n",
      "       181.0       0.18      0.29      0.23        85\n",
      "       182.0       0.02      0.01      0.02        69\n",
      "       183.0       0.26      0.10      0.14        63\n",
      "       184.0       0.00      0.00      0.00        69\n",
      "       185.0       0.00      0.00      0.00        56\n",
      "       186.0       0.12      0.04      0.06        69\n",
      "       187.0       0.00      0.00      0.00        60\n",
      "       188.0       0.00      0.00      0.00        80\n",
      "       189.0       0.03      0.01      0.02        76\n",
      "       190.0       0.11      0.09      0.10        76\n",
      "       191.0       0.00      0.00      0.00        79\n",
      "       192.0       0.05      0.07      0.06       120\n",
      "       193.0       0.05      0.03      0.03        78\n",
      "       194.0       0.00      0.00      0.00        56\n",
      "       195.0       0.06      0.03      0.04        77\n",
      "       196.0       0.00      0.00      0.00        77\n",
      "       197.0       0.07      0.04      0.05        74\n",
      "       198.0       0.00      0.00      0.00        70\n",
      "       199.0       0.10      0.03      0.04        72\n",
      "       200.0       0.15      0.08      0.10        52\n",
      "       201.0       0.16      0.14      0.15        64\n",
      "       202.0       0.17      0.11      0.13        62\n",
      "       203.0       0.29      0.07      0.11        59\n",
      "       204.0       0.06      0.04      0.04        53\n",
      "       205.0       0.00      0.00      0.00        68\n",
      "       206.0       0.05      0.01      0.02        92\n",
      "       207.0       0.05      0.03      0.03        76\n",
      "       208.0       0.00      0.00      0.00        74\n",
      "       209.0       0.02      0.01      0.02        70\n",
      "       210.0       0.03      0.02      0.02        64\n",
      "       211.0       0.22      0.15      0.18       107\n",
      "       212.0       0.09      0.05      0.07        77\n",
      "       213.0       0.29      0.24      0.26       104\n",
      "       214.0       0.10      0.03      0.05        59\n",
      "       215.0       0.05      0.05      0.05        59\n",
      "       216.0       0.09      0.12      0.10        68\n",
      "       217.0       0.16      0.10      0.12        52\n",
      "       218.0       0.16      0.05      0.07        64\n",
      "       219.0       0.24      0.16      0.19        64\n",
      "       220.0       0.00      0.00      0.00        73\n",
      "       221.0       0.12      0.03      0.05        65\n",
      "       222.0       0.43      0.05      0.09        58\n",
      "       223.0       0.24      0.35      0.28        79\n",
      "       224.0       0.24      0.26      0.25        61\n",
      "       225.0       0.00      0.00      0.00        66\n",
      "       226.0       0.14      0.09      0.11       112\n",
      "       227.0       0.04      0.01      0.02        68\n",
      "       228.0       0.00      0.00      0.00        64\n",
      "       229.0       0.48      0.37      0.42        71\n",
      "       230.0       0.08      0.12      0.10        76\n",
      "       231.0       0.21      0.44      0.28       259\n",
      "       232.0       0.00      0.00      0.00        78\n",
      "       233.0       0.16      0.12      0.14        85\n",
      "       234.0       0.15      0.09      0.11        77\n",
      "       235.0       0.04      0.03      0.03        61\n",
      "       236.0       0.22      0.23      0.23        56\n",
      "       237.0       0.22      0.19      0.21        67\n",
      "       238.0       0.25      0.19      0.21        59\n",
      "       239.0       0.26      0.73      0.39       132\n",
      "       240.0       0.03      0.02      0.02        66\n",
      "       241.0       0.00      0.00      0.00        63\n",
      "       242.0       0.00      0.00      0.00        61\n",
      "       243.0       0.04      0.05      0.05        57\n",
      "       244.0       0.19      0.05      0.07        66\n",
      "       245.0       0.11      0.09      0.10        67\n",
      "       246.0       0.00      0.00      0.00        66\n",
      "       247.0       0.70      0.10      0.18        67\n",
      "       248.0       0.00      0.00      0.00        68\n",
      "       249.0       0.00      0.00      0.00        57\n",
      "       250.0       0.58      0.94      0.72       568\n",
      "       251.0       0.48      0.64      0.55        81\n",
      "       252.0       0.64      0.99      0.78       316\n",
      "       253.0       0.00      0.00      0.00        67\n",
      "       254.0       0.10      0.01      0.02        82\n",
      "       255.0       0.03      0.03      0.03        70\n",
      "       256.0       0.10      0.51      0.17       548\n",
      "\n",
      "    accuracy                           0.16     21425\n",
      "   macro avg       0.11      0.09      0.09     21425\n",
      "weighted avg       0.14      0.16      0.13     21425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
